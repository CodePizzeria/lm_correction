{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPSsg2S-Z7w-",
    "outputId": "ce769cd0-9f35-441f-a703-ad0b63c5f4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import train_utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q4-RGh-ObXdW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n",
      "x\n",
      " [[ 9 23  2  7 23  9  2  8  9 24]\n",
      " [24 22 22 10 21 25 25  9 25 11]\n",
      " [ 9 24 23 24 10 15  0 11 25  1]\n",
      " [16  9 24 22 24  1  9 14  7  3]\n",
      " [25  9 12  0 21 16  9 25  0  2]\n",
      " [22  9 24 18 25  2  9 15  2 18]\n",
      " [25 12  9 11 23  9 23  2 10 12]\n",
      " [ 9 25  2  1 21 12 11  1 21 25]\n",
      " [10  9  8 10  2  1  9 14 24 10]\n",
      " [11  1  3  7 10 20 21 10  9  2]]\n",
      "\n",
      "y\n",
      " [[23  2  7 23  9  2  8  9 24  9]\n",
      " [22 22 10 21 25 25  9 25 11 19]\n",
      " [24 23 24 10 15  0 11 25  1  9]\n",
      " [ 9 24 22 24  1  9 14  7  3 18]\n",
      " [ 9 12  0 21 16  9 25  0  2 13]\n",
      " [ 9 24 18 25  2  9 15  2 18 18]\n",
      " [12  9 11 23  9 23  2 10 12  0]\n",
      " [25  2  1 21 12 11  1 21 25  9]\n",
      " [ 9  8 10  2  1  9 14 24 10 12]\n",
      " [ 1  3  7 10 20 21 10  9  2 10]]\n"
     ]
    }
   ],
   "source": [
    "# read in the extracted text file\n",
    "with open('datasets/text8_train') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])\n",
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "batches = train_utils.get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "# this is what the batches look like, note that y is just x shifted back by one. \n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wmXnpVC_I3Ay"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        # Creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        ## Define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        ## Define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        ## Define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network.\n",
    "            These inputs are x, and the hidden/cell state `hc`. '''\n",
    "\n",
    "        ## Get x, and the new hidden state (h, c) from the lstm\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "\n",
    "        ## Ppass x through the dropout layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Stack up LSTM outputs using view\n",
    "        x = x.reshape(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "\n",
    "        ## Put x through the fully-connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # Return x and the hidden state (h, c)\n",
    "        return x, (h, c)\n",
    "\n",
    "\n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "\n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "\n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "\n",
    "        inputs = torch.from_numpy(x)\n",
    "\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "        p = p.numpy().squeeze()\n",
    "\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "        return self.int2char[char], h\n",
    "\n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "\n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "\n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "J4t12jCLJF0h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(27, 512, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=27, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(net)\n\u001b[1;32m      7\u001b[0m n_seqs, n_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_seqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EMG/lm_correction/train_utils.py:76\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, n_seqs, n_steps, lr, clip, val_frac, cuda, print_every)\u001b[0m\n\u001b[1;32m     72\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     74\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m---> 76\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# create training and validation data\u001b[39;00m\n\u001b[1;32m     79\u001b[0m val_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mval_frac))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "if 'net' in locals():\n",
    "    del net\n",
    "\n",
    "net = CharRNN(chars, n_hidden=512, n_layers=1)\n",
    "print(net)\n",
    "\n",
    "n_seqs, n_steps = 128, 100\n",
    "train_utils.train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mCxrcfe5xhP"
   },
   "outputs": [],
   "source": [
    "\n",
    "def letter_to_emg_sim(key, char_tuple, noise_dist= 1, typing_style='skilled'):\n",
    "\n",
    "  keyboard = {0:'qwertyuiop',1:'asdfghjkl',2:'zxcvbnm',3:' '}\n",
    "\n",
    "  int2char = dict(enumerate(char_tuple))\n",
    "  char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "\n",
    "\n",
    "  # find the location of key on physical keyboard\n",
    "  for row in range(4):\n",
    "    argmax_key_column = keyboard[row].rfind(key)\n",
    "    if argmax_key_column != -1:\n",
    "      argmax_key_row = row\n",
    "      break\n",
    "  # create some parameters for archetypal typists\n",
    "  if typing_style == 'skilled':\n",
    "    accuracy = .5\n",
    "    softmax_range = 1 # keys\n",
    "  if typing_style == 'unskilled':\n",
    "    accuracy = .25\n",
    "    softmax_range = 2 # keys\n",
    "\n",
    "  # set the peak probability at the true key \"accuracy\"% of the time,\n",
    "  # otherwise it is uniformly randomly assigned to a key less than \"softmax_range\" keys away\n",
    "  if np.random.rand() > accuracy:\n",
    "    r_shift = np.random.choice([ i  for i in range(-softmax_range,softmax_range+1) if i != 0])\n",
    "    c_shift = np.random.choice([ i  for i in range(-softmax_range,softmax_range+1) if i != 0])\n",
    "    while keyboard_index_is_lowercaseletter(argmax_key_row+r_shift,argmax_key_column+c_shift) is False:\n",
    "      r_shift = np.random.choice([ i  for i in range(-softmax_range,softmax_range+1) if i != 0])\n",
    "      c_shift = np.random.choice([ i  for i in range(-softmax_range,softmax_range+1) if i != 0])\n",
    "    argmax_key_row = argmax_key_row + r_shift\n",
    "    argmax_key_column = argmax_key_column + c_shift\n",
    "  max_key = keyboard[argmax_key_row][argmax_key_column]\n",
    "\n",
    "  p = np.zeros((len(char_tuple)))\n",
    "  # space key has no errors\n",
    "  if key == ' ':\n",
    "    # make the space key correct 80% of the time.\n",
    "\n",
    "    for char in ['c','v','b','n','m']:\n",
    "      p[char2int[char]] = np.random.random()\n",
    "\n",
    "    p[char2int[key]] = np.random.random()+.65 # 80% correct space bar\n",
    "\n",
    "    return p/np.sum(p)\n",
    "\n",
    "  # add noise to softmax for keys within \"softmax_range\" of the peak prob key\n",
    "  for i in range(-softmax_range, softmax_range+1):\n",
    "    for j in range(-softmax_range, softmax_range+1):\n",
    "      if not keyboard_index_is_lowercaseletter(argmax_key_row+i, argmax_key_column+j):\n",
    "        continue\n",
    "      # add the noise to the element in p corresponding to the key\n",
    "      noise_key = keyboard[argmax_key_row+i][argmax_key_column+j]\n",
    "      distance = np.max([abs(i),abs(j)])\n",
    "      noise = 2*np.random.random()-1\n",
    "      p[char2int[noise_key]] = ((softmax_range-distance+1) + noise) /(softmax_range+1)\n",
    "\n",
    "  p[char2int[max_key]] = 1\n",
    "  return p/np.sum(p)\n",
    "\n",
    "def keyboard_index_is_lowercaseletter(row_index, column_index):\n",
    "  # top, left, and bottom of keyboard cases\n",
    "\n",
    "  if row_index < 0 or row_index > 2 or column_index < 0 :\n",
    "    return False\n",
    "  # right boundaries, manually defined for each row\n",
    "  if row_index == 0 and column_index > 9:\n",
    "    return False\n",
    "  if row_index == 1 and column_index > 8:\n",
    "    return False\n",
    "  if row_index == 2 and column_index > 6:\n",
    "    return False\n",
    "  return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7qYxtCQbzCF"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/UCLA/Courses/NLP/CS 263 Final Project/text8_train') as f:\n",
    "    text_train = f.read()\n",
    "text_train_noised = ''\n",
    "for char in text_train:\n",
    "  if len(text_train_noised)%300000 ==0:\n",
    "    print(len(text_train_noised)/80000000)\n",
    "    with open('/content/drive/MyDrive/UCLA/Courses/NLP/CS 263 Final Project/text8_train_noised','w') as f:\n",
    "      f.write(text_train_noised)\n",
    "  text_train_noised = text_train_noised+net.chars[np.argmax(letter_to_emg_sim(char, net.chars, typing_style='skilled'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GiBLZgbre9x9",
    "outputId": "b071f7a0-f4ef-4245-d2dd-f2ea20425902"
   },
   "outputs": [],
   "source": [
    "\n",
    "# with open('/content/drive/MyDrive/UCLA/Courses/NLP/CS 263 Final Project/text8') as f:\n",
    "#     text = f.read()\n",
    "# with open('/content/drive/MyDrive/UCLA/Courses/NLP/CS 263 Final Project/text8_train','w') as f:\n",
    "#     f.write(text[:80000000])\n",
    "with open('/content/drive/MyDrive/UCLA/Courses/NLP/CS 263 Final Project/text8_train') as f:\n",
    "    text_train = f.read()\n",
    "print(len(text_train))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "emg",
   "language": "python",
   "name": "emg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
