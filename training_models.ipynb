{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPSsg2S-Z7w-",
    "outputId": "ce769cd0-9f35-441f-a703-ad0b63c5f4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import train_utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "q4-RGh-ObXdW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n",
      "x\n",
      " [[11  1  3  7 10 20 21 10  9  2]\n",
      " [10  9  8 10  2  1  9 14 24 10]\n",
      " [24 22 22 10 21 25 25  9 25 11]\n",
      " [22  9 24 18 25  2  9 15  2 18]\n",
      " [16  9 24 22 24  1  9 14  7  3]\n",
      " [25  9 12  0 21 16  9 25  0  2]\n",
      " [ 9 24 23 24 10 15  0 11 25  1]\n",
      " [ 9 25  2  1 21 12 11  1 21 25]\n",
      " [25 12  9 11 23  9 23  2 10 12]\n",
      " [ 9 23  2  7 23  9  2  8  9 24]]\n",
      "\n",
      "y\n",
      " [[ 1  3  7 10 20 21 10  9  2 10]\n",
      " [ 9  8 10  2  1  9 14 24 10 12]\n",
      " [22 22 10 21 25 25  9 25 11 19]\n",
      " [ 9 24 18 25  2  9 15  2 18 18]\n",
      " [ 9 24 22 24  1  9 14  7  3 18]\n",
      " [ 9 12  0 21 16  9 25  0  2 13]\n",
      " [24 23 24 10 15  0 11 25  1  9]\n",
      " [25  2  1 21 12 11  1 21 25  9]\n",
      " [12  9 11 23  9 23  2 10 12  0]\n",
      " [23  2  7 23  9  2  8  9 24  9]]\n"
     ]
    }
   ],
   "source": [
    "# read in the extracted text file\n",
    "with open('datasets/text8_train') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])\n",
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "batches = train_utils.get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "# this is what the batches look like, note that y is just x shifted back by one. \n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000000\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "J4t12jCLJF0h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(27, 512, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=27, bias=True)\n",
      ")\n",
      "Epoch: 1/25... Step: 1... Loss: 3.3429... Val Loss: 3.1130\n",
      "Epoch: 1/25... Step: 2001... Loss: 1.5485... Val Loss: 1.5333\n",
      "Epoch: 1/25... Step: 4001... Loss: 1.4159... Val Loss: 1.4144\n",
      "Epoch: 2/25... Step: 6001... Loss: 1.2993... Val Loss: 1.3627\n",
      "Epoch: 2/25... Step: 8001... Loss: 1.3355... Val Loss: 1.3317\n",
      "Epoch: 2/25... Step: 10001... Loss: 1.2878... Val Loss: 1.3105\n",
      "Epoch: 3/25... Step: 12001... Loss: 1.2880... Val Loss: 1.2949\n",
      "Epoch: 3/25... Step: 14001... Loss: 1.2488... Val Loss: 1.2865\n",
      "Epoch: 3/25... Step: 16001... Loss: 1.3048... Val Loss: 1.2762\n",
      "Epoch: 4/25... Step: 18001... Loss: 1.2987... Val Loss: 1.2644\n",
      "Epoch: 4/25... Step: 20001... Loss: 1.2651... Val Loss: 1.2624\n",
      "Epoch: 4/25... Step: 22001... Loss: 1.2569... Val Loss: 1.2528\n",
      "Epoch: 5/25... Step: 24001... Loss: 1.1973... Val Loss: 1.2471\n",
      "Epoch: 5/25... Step: 26001... Loss: 1.2671... Val Loss: 1.2441\n",
      "Epoch: 5/25... Step: 28001... Loss: 1.2436... Val Loss: 1.2408\n",
      "Epoch: 6/25... Step: 30001... Loss: 1.2359... Val Loss: 1.2339\n",
      "Epoch: 6/25... Step: 32001... Loss: 1.2342... Val Loss: 1.2319\n",
      "Epoch: 7/25... Step: 34001... Loss: 1.1783... Val Loss: 1.2277\n",
      "Epoch: 7/25... Step: 36001... Loss: 1.2347... Val Loss: 1.2266\n",
      "Epoch: 7/25... Step: 38001... Loss: 1.2180... Val Loss: 1.2237\n",
      "Epoch: 8/25... Step: 40001... Loss: 1.1970... Val Loss: 1.2192\n",
      "Epoch: 8/25... Step: 42001... Loss: 1.2405... Val Loss: 1.2199\n",
      "Epoch: 8/25... Step: 44001... Loss: 1.1763... Val Loss: 1.2181\n",
      "Epoch: 9/25... Step: 46001... Loss: 1.1970... Val Loss: 1.2142\n",
      "Epoch: 9/25... Step: 48001... Loss: 1.2367... Val Loss: 1.2150\n",
      "Epoch: 9/25... Step: 50001... Loss: 1.1923... Val Loss: 1.2115\n",
      "Epoch: 10/25... Step: 52001... Loss: 1.1726... Val Loss: 1.2064\n",
      "Epoch: 10/25... Step: 54001... Loss: 1.2220... Val Loss: 1.2085\n",
      "Epoch: 10/25... Step: 56001... Loss: 1.2309... Val Loss: 1.2045\n",
      "Epoch: 11/25... Step: 58001... Loss: 1.1792... Val Loss: 1.2037\n",
      "Epoch: 11/25... Step: 60001... Loss: 1.2081... Val Loss: 1.2040\n",
      "Epoch: 12/25... Step: 62001... Loss: 1.2216... Val Loss: 1.2012\n",
      "Epoch: 12/25... Step: 64001... Loss: 1.2109... Val Loss: 1.1998\n",
      "Epoch: 12/25... Step: 66001... Loss: 1.2195... Val Loss: 1.1990\n",
      "Epoch: 13/25... Step: 68001... Loss: 1.2205... Val Loss: 1.1964\n",
      "Epoch: 13/25... Step: 70001... Loss: 1.1373... Val Loss: 1.1968\n",
      "Epoch: 13/25... Step: 72001... Loss: 1.2215... Val Loss: 1.1952\n",
      "Epoch: 14/25... Step: 74001... Loss: 1.1864... Val Loss: 1.1935\n",
      "Epoch: 14/25... Step: 76001... Loss: 1.1828... Val Loss: 1.1948\n",
      "Epoch: 14/25... Step: 78001... Loss: 1.2317... Val Loss: 1.1934\n",
      "Epoch: 15/25... Step: 80001... Loss: 1.1840... Val Loss: 1.1896\n",
      "Epoch: 15/25... Step: 82001... Loss: 1.1967... Val Loss: 1.1916\n",
      "Epoch: 15/25... Step: 84001... Loss: 1.1733... Val Loss: 1.1907\n",
      "Epoch: 16/25... Step: 86001... Loss: 1.1619... Val Loss: 1.1877\n",
      "Epoch: 16/25... Step: 88001... Loss: 1.1932... Val Loss: 1.1889\n",
      "Epoch: 17/25... Step: 90001... Loss: 1.2775... Val Loss: 1.1884\n",
      "Epoch: 17/25... Step: 92001... Loss: 1.1966... Val Loss: 1.1867\n",
      "Epoch: 17/25... Step: 94001... Loss: 1.1926... Val Loss: 1.1858\n",
      "Epoch: 18/25... Step: 96001... Loss: 1.1280... Val Loss: 1.1853\n",
      "Epoch: 18/25... Step: 98001... Loss: 1.1777... Val Loss: 1.1849\n",
      "Epoch: 18/25... Step: 100001... Loss: 1.1583... Val Loss: 1.1838\n",
      "Epoch: 19/25... Step: 102001... Loss: 1.1705... Val Loss: 1.1844\n",
      "Epoch: 19/25... Step: 104001... Loss: 1.1515... Val Loss: 1.1845\n",
      "Epoch: 19/25... Step: 106001... Loss: 1.1986... Val Loss: 1.1824\n",
      "Epoch: 20/25... Step: 108001... Loss: 1.2154... Val Loss: 1.1815\n",
      "Epoch: 20/25... Step: 110001... Loss: 1.1872... Val Loss: 1.1845\n",
      "Epoch: 20/25... Step: 112001... Loss: 1.1755... Val Loss: 1.1807\n",
      "Epoch: 21/25... Step: 114001... Loss: 1.1327... Val Loss: 1.1790\n",
      "Epoch: 21/25... Step: 116001... Loss: 1.2002... Val Loss: 1.1807\n",
      "Epoch: 21/25... Step: 118001... Loss: 1.1793... Val Loss: 1.1796\n",
      "Epoch: 22/25... Step: 120001... Loss: 1.1795... Val Loss: 1.1784\n",
      "Epoch: 22/25... Step: 122001... Loss: 1.1834... Val Loss: 1.1781\n",
      "Epoch: 23/25... Step: 124001... Loss: 1.1284... Val Loss: 1.1768\n",
      "Epoch: 23/25... Step: 126001... Loss: 1.1798... Val Loss: 1.1788\n",
      "Epoch: 23/25... Step: 128001... Loss: 1.1712... Val Loss: 1.1774\n",
      "Epoch: 24/25... Step: 130001... Loss: 1.1552... Val Loss: 1.1764\n",
      "Epoch: 24/25... Step: 132001... Loss: 1.1969... Val Loss: 1.1783\n",
      "Epoch: 24/25... Step: 134001... Loss: 1.1433... Val Loss: 1.1769\n",
      "Epoch: 25/25... Step: 136001... Loss: 1.1557... Val Loss: 1.1750\n",
      "Epoch: 25/25... Step: 138001... Loss: 1.1956... Val Loss: 1.1769\n",
      "Epoch: 25/25... Step: 140001... Loss: 1.1627... Val Loss: 1.1762\n"
     ]
    }
   ],
   "source": [
    "if 'net' in locals():\n",
    "    del net\n",
    "\n",
    "net = CharRNN(chars, n_hidden=512, n_layers=1)\n",
    "print(net)\n",
    "\n",
    "n_seqs, n_steps = 128, 100\n",
    "train_utils.train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mCxrcfe5xhP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7qYxtCQbzCF"
   },
   "outputs": [],
   "source": [
    "# old code used to add typos to the language generation training dataset. ignore this. \n",
    "with open('/content/drive/MyDrive/UCLA/Courses/NLP/CS 263 Final Project/text8_train') as f:\n",
    "    text_train = f.read()\n",
    "text_train_noised = ''\n",
    "for char in text_train:\n",
    "    if len(text_train_noised)%300000 ==0:\n",
    "        print(len(text_train_noised)/80000000)\n",
    "    with open('/content/drive/MyDrive/UCLA/Courses/NLP/CS 263 Final Project/text8_train_noised','w') as f:\n",
    "        f.write(text_train_noised)\n",
    "    text_train_noised = text_train_noised+net.chars[np.argmax(letter_to_emg_sim(char, net.chars, typing_style='skilled'))]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "emg",
   "language": "python",
   "name": "emg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
